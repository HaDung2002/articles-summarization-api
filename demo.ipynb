{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxlGoSwO8Xav",
        "outputId": "d295300b-c2f2-4653-bd78-c115ca94d182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CustomLLM/articles-summarization-api\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CustomLLM/articles-summarization-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtS3PvKYiQ_t",
        "outputId": "96e6fb67-ab47-41dd-c264-bf48a20a1eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (9.4.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!set LLAMA_CUBLAS=1\n",
        "!set CMAKE_ARGS=-DLLAMA_CUBLAS=on\n",
        "!set FORCE_CMAKE=1\n",
        "\n",
        "!python -m pip install llama-cpp-python==0.2.7 --prefer-binary --extra-index-url=https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122\n",
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "38Mc02_c8lcp",
        "outputId": "6e0df43f-12a4-4b82-f233-e91bf91718a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.7.0)\n",
            "Collecting anyio==4.4.0 (from -r requirements.txt (line 2))\n",
            "  Downloading anyio-4.4.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi==2022.12.7 (from -r requirements.txt (line 3))\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer==2.1.1 (from -r requirements.txt (line 4))\n",
            "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (8.1.7)\n",
            "Collecting colorama==0.4.6 (from -r requirements.txt (line 6))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: diskcache==5.6.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (5.6.3)\n",
            "Collecting dnspython==2.6.1 (from -r requirements.txt (line 8))\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator==2.1.1 (from -r requirements.txt (line 9))\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: exceptiongroup==1.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.2.1)\n",
            "Collecting filelock==3.13.1 (from -r requirements.txt (line 11))\n",
            "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Collecting fsspec==2024.6.0 (from -r requirements.txt (line 12))\n",
            "  Downloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.9/176.9 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11==0.14.0 (from -r requirements.txt (line 13))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore==1.0.5 (from -r requirements.txt (line 14))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools==0.6.1 (from -r requirements.txt (line 15))\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx==0.27.0 (from -r requirements.txt (line 16))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub==0.23.3 (from -r requirements.txt (line 17))\n",
            "  Downloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==3.4 (from -r requirements.txt (line 18))\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2==3.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (3.1.4)\n",
            "Requirement already satisfied: llama_cpp_python==0.2.7+cu122 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (0.2.7+cu122)\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe==2.1.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (2.1.5)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (0.1.2)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (1.3.0)\n",
            "Collecting networkx==3.2.1 (from -r requirements.txt (line 25))\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.26.4 (from -r requirements.txt (line 26))\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson==3.10.3 (from -r requirements.txt (line 27))\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging==24.0 (from -r requirements.txt (line 28))\n",
            "  Downloading packaging-24.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow==10.2.0 (from -r requirements.txt (line 29))\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==2.7.3 (from -r requirements.txt (line 30))\n",
            "  Downloading pydantic-2.7.3-py3-none-any.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic_core==2.18.4 (from -r requirements.txt (line 31))\n",
            "  Downloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pygments==2.18.0 (from -r requirements.txt (line 32))\n",
            "  Downloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv==1.0.1 (from -r requirements.txt (line 33))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting python-multipart==0.0.9 (from -r requirements.txt (line 34))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (6.0.1)\n",
            "Collecting requests==2.28.1 (from -r requirements.txt (line 36))\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich==13.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 37)) (13.7.1)\n",
            "Requirement already satisfied: shellingham==1.5.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 38)) (1.5.4)\n",
            "Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 39)) (1.3.1)\n",
            "Collecting starlette==0.37.2 (from -r requirements.txt (line 40))\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sympy==1.12 (from -r requirements.txt (line 41))\n",
            "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==2.3.0+cu121 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchaudio==2.3.0+cu121 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 43)) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision==0.18.0+cu121 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 44)) (0.18.0+cu121)\n",
            "Requirement already satisfied: tqdm==4.66.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 45)) (4.66.4)\n",
            "Requirement already satisfied: typer==0.12.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 46)) (0.12.3)\n",
            "Collecting typing_extensions==4.12.1 (from -r requirements.txt (line 47))\n",
            "  Downloading typing_extensions-4.12.1-py3-none-any.whl (37 kB)\n",
            "Collecting ujson==5.10.0 (from -r requirements.txt (line 48))\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles==0.22.0 (from -r requirements.txt (line 49))\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi (from -r requirements.txt (line 50))\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn (from -r requirements.txt (line 51))\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 52)) (1.6.0)\n",
            "Collecting pyngrok (from -r requirements.txt (line 53))\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1 (from requests==2.28.1->-r requirements.txt (line 36))\n",
            "  Downloading urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r requirements.txt (line 42)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r requirements.txt (line 42)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r requirements.txt (line 42)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r requirements.txt (line 42)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r requirements.txt (line 42)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r requirements.txt (line 42)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r requirements.txt (line 42)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r requirements.txt (line 42)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r requirements.txt (line 42)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r requirements.txt (line 42)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r requirements.txt (line 42)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0+cu121->-r requirements.txt (line 42)) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0+cu121->-r requirements.txt (line 42)) (12.1.105)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi->-r requirements.txt (line 50))\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn->-r requirements.txt (line 51))\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn->-r requirements.txt (line 51))\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, uvloop, urllib3, ujson, typing_extensions, sympy, python-multipart, python-dotenv, pyngrok, Pygments, pillow, packaging, orjson, numpy, networkx, idna, httptools, h11, fsspec, filelock, dnspython, colorama, charset-normalizer, certifi, uvicorn, requests, pydantic_core, httpcore, email_validator, anyio, watchfiles, starlette, pydantic, huggingface-hub, httpx, fastapi-cli, fastapi\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.12.1\n",
            "    Uninstalling sympy-1.12.1:\n",
            "      Successfully uninstalled sympy-1.12.1\n",
            "  Attempting uninstall: Pygments\n",
            "    Found existing installation: Pygments 2.16.1\n",
            "    Uninstalling Pygments-2.16.1:\n",
            "      Successfully uninstalled Pygments-2.16.1\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.6.0\n",
            "    Uninstalling fsspec-2023.6.0:\n",
            "      Successfully uninstalled fsspec-2023.6.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.15.4\n",
            "    Uninstalling filelock-3.15.4:\n",
            "      Successfully uninstalled filelock-3.15.4\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2024.6.2\n",
            "    Uninstalling certifi-2024.6.2:\n",
            "      Successfully uninstalled certifi-2024.6.2\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pydantic_core\n",
            "    Found existing installation: pydantic_core 2.20.0\n",
            "    Uninstalling pydantic_core-2.20.0:\n",
            "      Successfully uninstalled pydantic_core-2.20.0\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.8.0\n",
            "    Uninstalling pydantic-2.8.0:\n",
            "      Successfully uninstalled pydantic-2.8.0\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.23.4\n",
            "    Uninstalling huggingface-hub-0.23.4:\n",
            "      Successfully uninstalled huggingface-hub-0.23.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2024.6.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.28.1 which is incompatible.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.4.0 which is incompatible.\n",
            "kaggle 1.6.14 requires certifi>=2023.7.22, but you have certifi 2022.12.7 which is incompatible.\n",
            "yfinance 0.2.40 requires requests>=2.31, but you have requests 2.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pygments-2.18.0 anyio-4.4.0 certifi-2022.12.7 charset-normalizer-2.1.1 colorama-0.4.6 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 filelock-3.13.1 fsspec-2024.6.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 huggingface-hub-0.23.3 idna-3.4 networkx-3.2.1 numpy-1.26.4 orjson-3.10.3 packaging-24.0 pillow-10.2.0 pydantic-2.7.3 pydantic_core-2.18.4 pyngrok-7.1.6 python-dotenv-1.0.1 python-multipart-0.0.9 requests-2.28.1 starlette-0.37.2 sympy-1.12 typing_extensions-4.12.1 ujson-5.10.0 urllib3-1.26.19 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-12.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "1407531e7d2647f3a5c07b597b2bc66f",
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xsj6kY2UEBc9",
        "outputId": "5d4642ba-ca24-4375-8de7-bfcb528c697a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "Fetching 1 files:   0% 0/1 [00:00<?, ?it/s]Downloading 'Mistral-7B-Instruct-v0.3-Q4_K_M.gguf' to '.huggingface/download/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf.1270d22c0fbb3d092fb725d4d96c457b7b687a5f5a715abe1e818da303e562b6.incomplete'\n",
            "\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:   0% 0.00/4.37G [00:00<?, ?B/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:   1% 41.9M/4.37G [00:00<00:10, 408MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:   2% 105M/4.37G [00:00<00:08, 487MB/s] \u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:   4% 168M/4.37G [00:00<00:08, 517MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:   5% 220M/4.37G [00:00<00:10, 404MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:   6% 273M/4.37G [00:00<00:09, 435MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:   7% 325M/4.37G [00:00<00:09, 444MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:   9% 377M/4.37G [00:00<00:09, 440MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  10% 430M/4.37G [00:00<00:09, 431MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  11% 493M/4.37G [00:01<00:08, 457MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  12% 545M/4.37G [00:01<00:08, 441MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  14% 598M/4.37G [00:01<00:08, 452MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  15% 650M/4.37G [00:01<00:08, 456MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  16% 703M/4.37G [00:01<00:07, 465MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  17% 755M/4.37G [00:01<00:07, 464MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  18% 807M/4.37G [00:01<00:09, 359MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  19% 849M/4.37G [00:02<00:10, 343MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  20% 891M/4.37G [00:02<00:10, 340MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  22% 944M/4.37G [00:02<00:09, 375MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  23% 996M/4.37G [00:02<00:08, 406MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  24% 1.05G/4.37G [00:02<00:08, 396MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  25% 1.09G/4.37G [00:02<00:08, 396MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  26% 1.13G/4.37G [00:02<00:08, 400MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  27% 1.17G/4.37G [00:02<00:08, 399MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  28% 1.22G/4.37G [00:02<00:07, 400MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  29% 1.27G/4.37G [00:03<00:07, 403MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  30% 1.31G/4.37G [00:03<00:07, 407MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  31% 1.36G/4.37G [00:03<00:06, 438MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  32% 1.42G/4.37G [00:03<00:06, 433MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  34% 1.47G/4.37G [00:03<00:08, 356MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  35% 1.51G/4.37G [00:03<00:07, 360MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  35% 1.55G/4.37G [00:03<00:07, 370MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  36% 1.59G/4.37G [00:03<00:08, 345MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  37% 1.64G/4.37G [00:04<00:08, 336MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  39% 1.69G/4.37G [00:04<00:07, 373MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  40% 1.73G/4.37G [00:04<00:06, 381MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  41% 1.78G/4.37G [00:04<00:06, 419MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  42% 1.84G/4.37G [00:04<00:05, 424MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  43% 1.89G/4.37G [00:04<00:06, 409MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  44% 1.94G/4.37G [00:04<00:05, 431MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  46% 1.99G/4.37G [00:04<00:05, 435MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  47% 2.04G/4.37G [00:05<00:06, 343MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  48% 2.10G/4.37G [00:05<00:06, 373MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  49% 2.15G/4.37G [00:05<00:05, 400MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  50% 2.20G/4.37G [00:05<00:05, 425MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  52% 2.25G/4.37G [00:05<00:05, 413MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  53% 2.31G/4.37G [00:05<00:04, 416MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  54% 2.36G/4.37G [00:05<00:04, 424MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  55% 2.41G/4.37G [00:05<00:04, 423MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  56% 2.46G/4.37G [00:06<00:04, 433MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  58% 2.52G/4.37G [00:06<00:04, 451MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  59% 2.57G/4.37G [00:06<00:03, 465MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  60% 2.62G/4.37G [00:06<00:03, 468MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  61% 2.67G/4.37G [00:06<00:03, 470MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  62% 2.73G/4.37G [00:06<00:03, 478MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  64% 2.78G/4.37G [00:06<00:03, 475MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  65% 2.83G/4.37G [00:06<00:03, 445MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  66% 2.88G/4.37G [00:06<00:03, 426MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  67% 2.94G/4.37G [00:07<00:03, 427MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  68% 2.99G/4.37G [00:07<00:03, 434MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  70% 3.04G/4.37G [00:07<00:03, 436MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  71% 3.09G/4.37G [00:07<00:03, 400MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  72% 3.14G/4.37G [00:07<00:03, 395MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  73% 3.18G/4.37G [00:07<00:03, 397MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  74% 3.22G/4.37G [00:07<00:03, 356MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  75% 3.26G/4.37G [00:07<00:03, 369MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  76% 3.31G/4.37G [00:08<00:02, 394MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  77% 3.37G/4.37G [00:08<00:02, 417MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  78% 3.42G/4.37G [00:08<00:02, 402MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  79% 3.47G/4.37G [00:08<00:02, 416MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  81% 3.52G/4.37G [00:08<00:01, 440MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  82% 3.58G/4.37G [00:08<00:01, 442MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  83% 3.63G/4.37G [00:08<00:01, 432MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  84% 3.68G/4.37G [00:08<00:01, 408MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  85% 3.73G/4.37G [00:09<00:01, 428MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  87% 3.79G/4.37G [00:09<00:01, 385MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  88% 3.83G/4.37G [00:09<00:01, 389MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  88% 3.87G/4.37G [00:09<00:01, 390MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  90% 3.92G/4.37G [00:09<00:01, 421MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  91% 3.97G/4.37G [00:09<00:00, 446MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  92% 4.03G/4.37G [00:09<00:00, 450MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  93% 4.08G/4.37G [00:09<00:00, 440MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  95% 4.14G/4.37G [00:09<00:00, 463MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  96% 4.19G/4.37G [00:10<00:00, 431MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  97% 4.25G/4.37G [00:10<00:00, 405MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf:  98% 4.30G/4.37G [00:10<00:00, 407MB/s]\u001b[A\n",
            "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf: 100% 4.37G/4.37G [00:10<00:00, 414MB/s]\n",
            "Download complete. Moving file to Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\n",
            "Fetching 1 files: 100% 1/1 [00:10<00:00, 10.96s/it]\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli download bartowski/Mistral-7B-Instruct-v0.3-GGUF --include \"Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\" --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0j-pvTy8uTR",
        "outputId": "2fcc0252-16c3-4acb-f549-f274d8e86f67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7txLUNY8wzR",
        "outputId": "b6d6a2c4-1a9b-455b-ef63-75fd40fbbe5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: NVIDIA L4, compute capability 8.9\n",
            "llama_model_loader: loaded meta data with 29 key-value pairs and 291 tensors from /content/drive/MyDrive/CustomLLM/articles-summarization-api/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf (version unknown)\n",
            "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32768,     1,     1 ]\n",
            "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   19:           blk.10.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   20:             blk.10.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   21:             blk.10.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   22:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   23:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   24:             blk.10.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   25:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   27:            blk.2.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   28:              blk.2.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   30:              blk.2.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   31:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   32:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   33:              blk.2.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   34:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   36:            blk.3.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   37:              blk.3.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   39:              blk.3.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   40:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   41:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   42:              blk.3.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   43:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   45:            blk.4.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   46:              blk.4.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   48:              blk.4.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   49:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   50:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   51:              blk.4.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   52:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   54:            blk.5.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   55:              blk.5.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   57:              blk.5.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   58:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   59:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   60:              blk.5.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   61:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   63:            blk.6.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   64:              blk.6.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   66:              blk.6.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   67:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   68:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   69:              blk.6.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   70:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   72:            blk.7.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   73:              blk.7.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   75:              blk.7.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   76:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   77:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   78:              blk.7.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   79:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   81:            blk.8.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   82:              blk.8.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   84:              blk.8.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   85:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   86:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   87:              blk.8.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   88:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   90:            blk.9.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   91:              blk.9.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   93:              blk.9.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   94:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   95:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   96:              blk.9.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   97:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  100:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  101:           blk.11.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  102:           blk.11.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  103:             blk.11.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  104:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  105:             blk.11.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  106:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  107:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  108:             blk.11.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  109:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  110:           blk.12.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  111:           blk.12.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  112:             blk.12.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  113:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  114:             blk.12.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  115:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  116:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  117:             blk.12.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  118:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  119:           blk.13.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  120:           blk.13.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  121:             blk.13.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  122:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  123:             blk.13.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  124:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  125:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  126:             blk.13.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  127:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  128:           blk.14.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  129:           blk.14.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  130:             blk.14.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  131:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  132:             blk.14.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  133:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  134:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  135:             blk.14.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  136:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  137:           blk.15.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  138:           blk.15.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  139:             blk.15.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  140:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  141:             blk.15.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  142:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  143:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  144:             blk.15.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  145:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  146:           blk.16.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  147:           blk.16.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  148:             blk.16.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  149:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  150:             blk.16.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  151:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  152:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  153:             blk.16.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  154:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  155:           blk.17.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  156:           blk.17.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  157:             blk.17.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  158:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  159:             blk.17.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  160:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  161:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  162:             blk.17.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  163:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  164:           blk.18.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  165:           blk.18.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  166:             blk.18.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  167:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  168:             blk.18.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  169:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  170:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  171:             blk.18.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  172:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  173:           blk.19.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  174:           blk.19.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  175:             blk.19.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  176:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  177:             blk.19.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  178:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  179:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  180:             blk.19.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  181:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  182:           blk.20.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  183:           blk.20.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  184:             blk.20.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  185:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  186:             blk.20.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  187:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  188:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  189:             blk.20.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  190:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  191:           blk.21.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  192:           blk.21.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  193:             blk.21.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  194:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  195:             blk.21.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  196:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  197:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  198:             blk.21.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  199:             blk.22.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  200:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  202:             blk.22.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  203:                    output.weight q6_K     [  4096, 32768,     1,     1 ]\n",
            "llama_model_loader: - tensor  204:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  205:           blk.22.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  206:           blk.22.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  208:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  209:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  210:           blk.23.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  211:           blk.23.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  212:             blk.23.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  213:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  214:             blk.23.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  215:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  216:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  217:             blk.23.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - kv   0:                       general.architecture str     \n",
            "llama_model_loader: - kv   1:                               general.name str     \n",
            "llama_model_loader: - kv   2:                          llama.block_count u32     \n",
            "llama_model_loader: - kv   3:                       llama.context_length u32     \n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32     \n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32     \n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32     \n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32     \n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
            "llama_model_loader: - kv  10:                          general.file_type u32     \n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32     \n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32     \n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str     \n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str     \n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr     \n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr     \n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr     \n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32     \n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32     \n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32     \n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool    \n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool    \n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str     \n",
            "llama_model_loader: - kv  24:               general.quantization_version u32     \n",
            "llama_model_loader: - kv  25:                      quantize.imatrix.file str     \n",
            "llama_model_loader: - kv  26:                   quantize.imatrix.dataset str     \n",
            "llama_model_loader: - kv  27:             quantize.imatrix.entries_count i32     \n",
            "llama_model_loader: - kv  28:              quantize.imatrix.chunks_count i32     \n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_print_meta: format         = unknown\n",
            "llm_load_print_meta: arch           = llama\n",
            "llm_load_print_meta: vocab type     = SPM\n",
            "llm_load_print_meta: n_vocab        = 32768\n",
            "llm_load_print_meta: n_merges       = 0\n",
            "llm_load_print_meta: n_ctx_train    = 32768\n",
            "llm_load_print_meta: n_ctx          = 2048\n",
            "llm_load_print_meta: n_embd         = 4096\n",
            "llm_load_print_meta: n_head         = 32\n",
            "llm_load_print_meta: n_head_kv      = 8\n",
            "llm_load_print_meta: n_layer        = 32\n",
            "llm_load_print_meta: n_rot          = 128\n",
            "llm_load_print_meta: n_gqa          = 4\n",
            "llm_load_print_meta: f_norm_eps     = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps = 1.0e-05\n",
            "llm_load_print_meta: n_ff           = 14336\n",
            "llm_load_print_meta: freq_base      = 10000.0\n",
            "llm_load_print_meta: freq_scale     = 1\n",
            "llm_load_print_meta: model type     = 7B\n",
            "llm_load_print_meta: model ftype    = mostly Q4_K - Medium\n",
            "llm_load_print_meta: model params   = 7.25 B\n",
            "llm_load_print_meta: model size     = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name   = Mistral-7B-Instruct-v0.3\n",
            "llm_load_print_meta: BOS token = 1 '<s>'\n",
            "llm_load_print_meta: EOS token = 2 '</s>'\n",
            "llm_load_print_meta: UNK token = 0 '<unk>'\n",
            "llm_load_print_meta: LF token  = 781 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.09 MB\n",
            "llm_load_tensors: using CUDA for GPU acceleration\n",
            "llm_load_tensors: mem required  =   72.09 MB (+  256.00 MB per state)\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloading v cache to GPU\n",
            "llm_load_tensors: offloading k cache to GPU\n",
            "llm_load_tensors: offloaded 35/35 layers to GPU\n",
            "llm_load_tensors: VRAM used: 4354 MB\n",
            "................................................................................................\n",
            "llama_new_context_with_model: kv self size  =  256.00 MB\n",
            "llama_new_context_with_model: compute buffer total size =  153.47 MB\n",
            "llama_new_context_with_model: VRAM scratch buffer: 152.00 MB\n",
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Public URL: https://0894-34-124-183-157.ngrok-free.app\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m2782\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\n",
            "llama_print_timings:        load time =   530.62 ms\n",
            "llama_print_timings:      sample time =   265.98 ms /   257 runs   (    1.03 ms per token,   966.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =  1389.74 ms /  1393 tokens (    1.00 ms per token,  1002.35 tokens per second)\n",
            "llama_print_timings:        eval time =  6477.80 ms /   256 runs   (   25.30 ms per token,    39.52 tokens per second)\n",
            "llama_print_timings:       total time =  8805.77 ms\n",
            "\u001b[32mINFO\u001b[0m:     203.205.32.65:0 - \"\u001b[1mPOST /chat/summary HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   530.62 ms\n",
            "llama_print_timings:      sample time =   682.32 ms /   659 runs   (    1.04 ms per token,   965.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =    92.09 ms /     3 tokens (   30.70 ms per token,    32.58 tokens per second)\n",
            "llama_print_timings:        eval time = 16919.69 ms /   658 runs   (   25.71 ms per token,    38.89 tokens per second)\n",
            "llama_print_timings:       total time = 19597.90 ms\n",
            "\u001b[32mINFO\u001b[0m:     203.205.32.65:0 - \"\u001b[1mPOST /chat/summary HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   530.62 ms\n",
            "llama_print_timings:      sample time =   554.29 ms /   535 runs   (    1.04 ms per token,   965.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    83.60 ms /     4 tokens (   20.90 ms per token,    47.85 tokens per second)\n",
            "llama_print_timings:        eval time = 13764.49 ms /   534 runs   (   25.78 ms per token,    38.80 tokens per second)\n",
            "llama_print_timings:       total time = 15881.38 ms\n",
            "\u001b[32mINFO\u001b[0m:     203.205.32.65:0 - \"\u001b[1mPOST /chat/summary HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   530.62 ms\n",
            "llama_print_timings:      sample time =   677.95 ms /   655 runs   (    1.04 ms per token,   966.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =  1320.53 ms /  1388 tokens (    0.95 ms per token,  1051.09 tokens per second)\n",
            "llama_print_timings:        eval time = 17072.88 ms /   654 runs   (   26.11 ms per token,    38.31 tokens per second)\n",
            "llama_print_timings:       total time = 20957.41 ms\n",
            "\u001b[32mINFO\u001b[0m:     203.205.32.65:0 - \"\u001b[1mPOST /chat/summary HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   530.62 ms\n",
            "llama_print_timings:      sample time =   339.30 ms /   326 runs   (    1.04 ms per token,   960.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =  8400.90 ms /   326 runs   (   25.77 ms per token,    38.81 tokens per second)\n",
            "llama_print_timings:       total time =  9602.53 ms\n",
            "\u001b[32mINFO\u001b[0m:     203.205.32.65:0 - \"\u001b[1mPOST /chat/summary HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "t=2024-07-10T07:39:14+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8000-63d5cad0-182c-4d00-8cfa-51947ae06859 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
            "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m2782\u001b[0m]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/CustomLLM/articles-summarization-api/server.py\", line 66, in <module>\n",
            "    uvicorn.run(app, port=8000)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/main.py\", line 577, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 65, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "  File \"uvloop/loop.pyx\", line 1511, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"uvloop/loop.pyx\", line 1504, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"uvloop/loop.pyx\", line 1377, in uvloop.loop.Loop.run_forever\n",
            "  File \"uvloop/loop.pyx\", line 555, in uvloop.loop.Loop._run\n",
            "  File \"uvloop/loop.pyx\", line 474, in uvloop.loop.Loop._on_idle\n",
            "  File \"uvloop/cbhandles.pyx\", line 83, in uvloop.loop.Handle._run\n",
            "  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 315, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 68, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 328, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python server.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
